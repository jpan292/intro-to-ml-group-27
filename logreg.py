# -*- coding: utf-8 -*-
"""logreg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qFHhzs-kKAXSf6wnP5G06QEB4z_lmbR0

# Users vs bots classification: Logistic Regression Model

## Outline of the what to do:
- Load the dataset
- Check for missing data or null value
- If there's any categorial feature, have to dummy code/ one hot encode
- Identify the target feature
- After that, set up a correlation matrix using heat map so see with features correlate with the target feature
- Select the features accordingly to the set threshold
- Maybe import pipeline and then do the regression?
- evaluate using different metric as well

## Load the dataset and display basic info
"""

#Import necessary libraries:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, classification_report)
import warnings
warnings.filterwarnings('ignore')

#Load the dataset:
df = pd.read_csv('/content/drive/MyDrive/DATASCI_3000/Project/bots_vs_users.csv')
df.head()

#Check basic stastics of the dataset
df.info()

#Check for leakage by using correlation matrix and eliminating abnormally high correlations (>=0.7)
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
corr = df[numeric_cols].corr()
plt.figure(figsize=(14, 10))
sns.heatmap(
    corr,
    annot=True,
    cmap='coolwarm',
    fmt=".2f",
    square=True,
    linewidths=.5,
    cbar=True
)
plt.title("Correlation Heatmap of Numeric Features")
plt.show()

"""## Preprocess, Split and Train the data"""

# Dropping trivial/ overly-dominant features that affect the model's performance:
dropped_features = ['has_short_name','has_first_name','city','is_blacklisted','has_photo', 'is_verified', 'is_confirmed',
    'can_post_on_wall', 'can_send_message',
    'can_add_as_friend', 'can_invite_to_group',
    'access_to_closed_profile', 'all_posts_visible',
    'has_maiden_name', 'has_military_service',
    'has_tv', 'has_books', 'has_quotes', 'has_music',
    'has_games', 'has_movies', 'has_activities',
    'has_interests', 'has_about', 'has_relatives',
    'has_schools', 'has_universities',
    'occupation_type_university', 'occupation_type_work','has_status', 'has_website', 'has_mobile', 'audio_available',
                    'has_nickname', 'has_last_name', 'has_occupation', 'has_domain']
df = df.drop(columns = dropped_features)
# Preprocess the data by handling the empty cells and cells that contain "Unknown" as a value
# Cannot drop empty cells because it is one of the signals indicating a bot account
# Strategy: replace empty cells with NaN
df.replace("",np.nan,inplace=True)

# Identify numerical features
num_cols = df.select_dtypes(include =['float64']).columns.tolist()

# subscribers_count feature is supposed to be a numerical feature but has "Unknown" cells
# strategy: convert "Uknown" to NaN and then append subscribers_count into numerical_cols
df['subscribers_count'] = df['subscribers_count'].replace("Unknown",np.nan)
df['subscribers_count'] = pd.to_numeric(df['subscribers_count'])
num_cols.append('subscribers_count')

# Categorical features are everything else except for the target feature
cat_cols = [c for c in df.columns if c not in num_cols and c != "target"]

# Add missingness indicators (in order to distinguish cells with actual values and empty cells)
for col in num_cols:
  df[col+'_missing'] = df[col].isnull().astype(int)

num_cols_with_indicator = num_cols + [col + '_missing' for col in num_cols]

# Preprocessing Transformers
# For numeric features, preprocess the data by median imputation (replace missing values with the median of the features)
numeric_transformer = Pipeline(steps = [
    ('imputer', SimpleImputer(strategy = 'median'))
])

# For categorical features use One Hot Encoding
categorical_transformer = Pipeline(steps = [
    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))
])

# Combining everything into a singular preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, num_cols),
        ('cat', categorical_transformer, cat_cols)
    ],
    remainder='drop'
)

# Logistic Regression Classifier model
logreg_classifier = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter = 500))
])

# Split the dataset into train/validate/test
X = df.drop('target', axis = 1)
y = df['target']

# Split training from test and val first
X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
# Split validation and test data up next
X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, test_size=2/3, random_state=42, stratify=y_test_val)

#Train the model
logreg_classifier.fit(X_train, y_train)

# Get feature names after preprocessing
feature_names = logreg_classifier.named_steps['preprocessor'].get_feature_names_out()

coef = pd.Series(logreg_classifier.named_steps['classifier'].coef_[0],
                 index=feature_names)
print(coef.sort_values(ascending=False))

"""## Evaluation"""

# Prediciton and evaluation for all models
models = {'Logistic Regression': logreg_classifier}

for model_name, model in models.items():
  ytrain_pred = model.predict(X_train)
  ytest_pred = model.predict(X_test)

  print(f"---{model_name}---")
  print("Test accuracy:", accuracy_score(y_test, ytest_pred))
  print("Precision:", precision_score(y_test, ytest_pred))
  print("Recall:", recall_score(y_test, ytest_pred))
  print("F1 score:", f1_score(y_test, ytest_pred))
  print("Confusion matrix:\n", confusion_matrix(y_test,ytest_pred))
  print("Classification Report: \n", classification_report(y_test,ytest_pred))

  #Calculate sensitivity and specificity
  # - True Positive: 1 predicted as 1
  # - True Negative: 0 predicted as 0
  # - False Positive: 1 predicted as 0
  # - False Negative: 0 predicted as 1
  tn, fp, fn, tp = confusion_matrix(y_test, ytest_pred).ravel()
  sensitivity = tp / (tp + fn) # recall
  specificity = tn / (tn + fp) # specificity
  print(f"Sensitivity (Recall) for {model_name}: {sensitivity:.4f}")
  print(f"Specificity for {model_name}: {specificity:.4f}")

  # Plot confusion matrix heatmap
  cm = confusion_matrix(y_test, ytest_pred)
  plt.figure()
  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])
  plt.title(f'Confusion Matrix for {model_name}')
  plt.xlabel('Predicted')
  plt.ylabel('Actual')
  plt.show()

  # ROC Curve and AUC
  ytest_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)
  fpr, tpr, thresholds = roc_curve(y_test, ytest_prob)
  roc_auc = roc_auc_score(y_test, ytest_prob)

  plt.figure()
  plt.plot(fpr, tpr, label = f'{model_name} (AUROC = {roc_auc:.2f})')
  plt.plot([0, 1], [0, 1], 'k--',color = 'gray')
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title(f'ROC Curve for {model_name}')
  plt.legend(loc = 'lower right')
  plt.show()